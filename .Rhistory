bagging_out_sample_pred2 = predict(bagging_model2, newdata=auctions.valb)
rf_in_sample_pred2 = predict(rf_model2, newdata=auctions.trainb)
rf_out_sample_pred2 = predict(rf_model2, newdata=auctions.valb)
xgb_in_sample_accuracy2 = sum(round(xgb_in_sample_pred2)==auctions.train$Competitive.)/length(xgb_in_sample_pred2)
xgb_out_sample_accuracy2 = sum(round(xgb_out_sample_pred2)==auctions.val$Competitive.)/length(xgb_out_sample_pred2)
xgb_imp_var2 = xgb.importance(model = xgb_model2)
bagging_in_sample_accuracy2 = sum(round(bagging_in_sample_pred2)==auctions.trainb$Competitive.)/length(bagging_in_sample_pred2)
bagging_out_sample_accuracy2 = sum(round(bagging_out_sample_pred2)==auctions.valb$Competitive.)/length(bagging_out_sample_pred2)
bagging_imp_var2 = varImp(bagging_model2)
rf_in_sample_accuracy2 = sum(round(rf_in_sample_pred2)==auctions.trainb$Competitive.)/length(rf_in_sample_pred2)
rf_out_sample_accuracy2 = sum(round(rf_out_sample_pred2)==auctions.valb$Competitive.)/length(rf_out_sample_pred2)
rf_imp_var2 = varImp(rf_model2)
columns_name = c("XGBoost", "Bagging", "Random forest")
train_accuracies2 = c(xgb_in_sample_accuracy2, bagging_in_sample_accuracy2, rf_in_sample_accuracy2)
val_accuracies2 = c(xgb_out_sample_accuracy2, bagging_out_sample_accuracy2, rf_out_sample_accuracy2)
important_var2 = c(
xgb_imp_var2$Feature[which.max(xgb_imp_var2$Gain)],
rownames(bagging_imp_var2)[which.max(bagging_imp_var2$Overall)],
rownames(rf_imp_var2)[which.max(rf_imp_var2$Overall)]
)
table2 = as.data.frame(t(data.frame(
Training_Accuracy = train_accuracies2,
Validation_Accuracy = val_accuracies2,
Most_Important_Variable = important_var2)))
colnames(table2) = columns_name
print(table2)
auctions.trainb = auctions.train[, -which(colnames(auctions.train) == "ClosePrice")]
auctions.valb = auctions.val[, -which(colnames(auctions.val) == "ClosePrice")]
xgb_model2 = xgboost(data = as.matrix(auctions.trainb[, -which(colnames(auctions.trainb) == "Competitive.")]),
label = auctions.trainb$Competitive.,
nrounds = 100,
objective = "binary:logistic",
eval_metric = "logloss",
max_depth = 6,
eta = 0.3)
bagging_model2 = bagging(Competitive. ~ ., data = auctions.trainb)
rf_model2 = randomForest(Competitive. ~., data = auctions.trainb)
xgb_in_sample_pred2 = predict(xgb_model2, newdata=as.matrix(auctions.trainb[, -which(colnames(auctions.trainb) == "Competitive.")]))
xgb_out_sample_pred2 = predict(xgb_model2, newdata=as.matrix(auctions.valb[, -which(colnames(auctions.valb) == "Competitive.")]))
bagging_in_sample_pred2 = predict(bagging_model2, newdata=auctions.trainb)
bagging_out_sample_pred2 = predict(bagging_model2, newdata=auctions.valb)
rf_in_sample_pred2 = predict(rf_model2, newdata=auctions.trainb)
rf_out_sample_pred2 = predict(rf_model2, newdata=auctions.valb)
xgb_in_sample_accuracy2 = sum(round(xgb_in_sample_pred2)==auctions.trainb$Competitive.)/length(xgb_in_sample_pred2)
xgb_out_sample_accuracy2 = sum(round(xgb_out_sample_pred2)==auctions.valb$Competitive.)/length(xgb_out_sample_pred2)
xgb_imp_var2 = xgb.importance(model = xgb_model2)
bagging_in_sample_accuracy2 = sum(round(bagging_in_sample_pred2)==auctions.trainb$Competitive.)/length(bagging_in_sample_pred2)
bagging_out_sample_accuracy2 = sum(round(bagging_out_sample_pred2)==auctions.valb$Competitive.)/length(bagging_out_sample_pred2)
bagging_imp_var2 = varImp(bagging_model2)
rf_in_sample_accuracy2 = sum(round(rf_in_sample_pred2)==auctions.trainb$Competitive.)/length(rf_in_sample_pred2)
rf_out_sample_accuracy2 = sum(round(rf_out_sample_pred2)==auctions.valb$Competitive.)/length(rf_out_sample_pred2)
rf_imp_var2 = varImp(rf_model2)
columns_name = c("XGBoost", "Bagging", "Random forest")
train_accuracies2 = c(xgb_in_sample_accuracy2, bagging_in_sample_accuracy2, rf_in_sample_accuracy2)
val_accuracies2 = c(xgb_out_sample_accuracy2, bagging_out_sample_accuracy2, rf_out_sample_accuracy2)
important_var2 = c(
xgb_imp_var2$Feature[which.max(xgb_imp_var2$Gain)],
rownames(bagging_imp_var2)[which.max(bagging_imp_var2$Overall)],
rownames(rf_imp_var2)[which.max(rf_imp_var2$Overall)]
)
table2 = as.data.frame(t(data.frame(
Training_Accuracy = train_accuracies2,
Validation_Accuracy = val_accuracies2,
Most_Important_Variable = important_var2)))
colnames(table2) = columns_name
print(table2)
auctions.trainc = auctions.train[, -which(colnames(auctions.train) == "OpenPrice")]
auctions.valc = auctions.val[, -which(colnames(auctions.val) == "OpenPrice")]
xgb_model3 = xgboost(data = as.matrix(auctions.trainc[, -which(colnames(auctions.trainc) == "Competitive.")]),
label = auctions.trainc$Competitive.,
nrounds = 100,
objective = "binary:logistic",
eval_metric = "logloss",
max_depth = 6,
eta = 0.3)
bagging_model3 = bagging(Competitive. ~ ., data = auctions.trainc)
rf_model3 = randomForest(Competitive. ~., data = auctions.trainc)
xgb_in_sample_pred3 = predict(xgb_model3, newdata=as.matrix(auctions.trainc[, -which(colnames(auctions.trainc) == "Competitive.")]))
xgb_out_sample_pred3 = predict(xgb_model3, newdata=as.matrix(auctions.valc[, -which(colnames(auctions.valc) == "Competitive.")]))
bagging_in_sample_pred3 = predict(bagging_model3, newdata=auctions.trainc)
bagging_out_sample_pred3 = predict(bagging_model3, newdata=auctions.valc)
rf_in_sample_pred3 = predict(rf_model3, newdata=auctions.trainc)
rf_out_sample_pred3 = predict(rf_model3, newdata=auctions.valc)
xgb_in_sample_accuracy3 = sum(round(xgb_in_sample_pred3)==auctions.trainc$Competitive.)/length(xgb_in_sample_pred3)
xgb_out_sample_accuracy3 = sum(round(xgb_out_sample_pred3)==auctions.valc$Competitive.)/length(xgb_out_sample_pred3)
xgb_imp_var3 = xgb.importance(model = xgb_model3)
bagging_in_sample_accuracy3 = sum(round(bagging_in_sample_pred3)==auctions.trainc$Competitive.)/length(bagging_in_sample_pred3)
bagging_out_sample_accuracy3 = sum(round(bagging_out_sample_pred3)==auctions.valc$Competitive.)/length(bagging_out_sample_pred3)
bagging_imp_var3 = varImp(bagging_model3)
rf_in_sample_accuracy3 = sum(round(rf_in_sample_pred3)==auctions.trainc$Competitive.)/length(rf_in_sample_pred3)
rf_out_sample_accuracy3 = sum(round(rf_out_sample_pred3)==auctions.valc$Competitive.)/length(rf_out_sample_pred3)
rf_imp_var3 = varImp(rf_model3)
columns_name = c("XGBoost", "Bagging", "Random forest")
train_accuracies3 = c(xgb_in_sample_accuracy3, bagging_in_sample_accuracy3, rf_in_sample_accuracy3)
val_accuracies3 = c(xgb_out_sample_accuracy3, bagging_out_sample_accuracy3, rf_out_sample_accuracy3)
important_var3 = c(
xgb_imp_var3$Feature[which.max(xgb_imp_var3$Gain)],
rownames(bagging_imp_var3)[which.max(bagging_imp_var3$Overall)],
rownames(rf_imp_var3)[which.max(rf_imp_var3$Overall)]
)
table3 = as.data.frame(t(data.frame(
Training_Accuracy = train_accuracies3,
Validation_Accuracy = val_accuracies3,
Most_Important_Variable = important_var3)))
colnames(table3) = columns_name
print(table3)
auctions.traind = auctions.train[, -which(colnames(auctions.train) == "OpenPrice", colnames(auctions.train) == "ClosePrice")]
auctions.vald = auctions.val[, -which(colnames(auctions.val) == "OpenPrice", colnames(auctions.train) == "ClosePrice")]
xgb_model4 = xgboost(data = as.matrix(auctions.traind[, -which(colnames(auctions.traind) == "Competitive.")]),
label = auctions.traind$Competitive.,
nrounds = 100,
objective = "binary:logistic",
eval_metric = "logloss",
max_depth = 6,
eta = 0.3)
bagging_model4 = bagging(Competitive. ~ ., data = auctions.traind)
rf_model4 = randomForest(Competitive. ~., data = auctions.traind)
xgb_in_sample_pred4 = predict(xgb_model4, newdata=as.matrix(auctions.traind[, -which(colnames(auctions.trainc) == "Competitive.")]))
xgb_out_sample_pred4 = predict(xgb_model4, newdata=as.matrix(auctions.vald[, -which(colnames(auctions.valc) == "Competitive.")]))
bagging_in_sample_pred4 = predict(bagging_model4, newdata=auctions.traind)
bagging_out_sample_pred4 = predict(bagging_model4, newdata=auctions.vald)
rf_in_sample_pred4 = predict(rf_model4, newdata=auctions.traind)
rf_out_sample_pred4 = predict(rf_model4, newdata=auctions.vald)
xgb_in_sample_accuracy4 = sum(round(xgb_in_sample_pred4)==auctions.traind$Competitive.)/length(xgb_in_sample_pred4)
xgb_out_sample_accuracy4 = sum(round(xgb_out_sample_pred4)==auctions.vald$Competitive.)/length(xgb_out_sample_pred4)
xgb_imp_var4 = xgb.importance(model = xgb_model4)
bagging_in_sample_accuracy4 = sum(round(bagging_in_sample_pred4)==auctions.traind$Competitive.)/length(bagging_in_sample_pred4)
bagging_out_sample_accuracy4 = sum(round(bagging_out_sample_pred4)==auctions.vald$Competitive.)/length(bagging_out_sample_pred4)
bagging_imp_var4 = varImp(bagging_model4)
rf_in_sample_accuracy4 = sum(round(rf_in_sample_pred4)==auctions.traind$Competitive.)/length(rf_in_sample_pred4)
rf_out_sample_accuracy4 = sum(round(rf_out_sample_pred4)==auctions.vald$Competitive.)/length(rf_out_sample_pred4)
rf_imp_var4 = varImp(rf_model4)
columns_name = c("XGBoost", "Bagging", "Random forest")
train_accuracies4 = c(xgb_in_sample_accuracy4, bagging_in_sample_accuracy4, rf_in_sample_accuracy4)
val_accuracies4 = c(xgb_out_sample_accuracy4, bagging_out_sample_accuracy4, rf_out_sample_accuracy4)
important_var4 = c(
xgb_imp_var4$Feature[which.max(xgb_imp_var4$Gain)],
rownames(bagging_imp_var4)[which.max(bagging_imp_var4$Overall)],
rownames(rf_imp_var4)[which.max(rf_imp_var4$Overall)]
)
table4 = as.data.frame(t(data.frame(
Training_Accuracy = train_accuracies4,
Validation_Accuracy = val_accuracies4,
Most_Important_Variable = important_var4)))
colnames(table4) = columns_name
print(table4)
auctions.traind = auctions.train[, -which(colnames(auctions.train) %in% c("OpenPrice", "ClosePrice"))]
auctions.vald = auctions.val[, -which(colnames(auctions.val) %in% c("OpenPrice", "ClosePrice"))]
xgb_model4 = xgboost(data = as.matrix(auctions.traind[, -which(colnames(auctions.traind) == "Competitive.")]),
label = auctions.traind$Competitive.,
nrounds = 100,
objective = "binary:logistic",
eval_metric = "logloss",
max_depth = 6,
eta = 0.3)
bagging_model4 = bagging(Competitive. ~ ., data = auctions.traind)
rf_model4 = randomForest(Competitive. ~., data = auctions.traind)
xgb_in_sample_pred4 = predict(xgb_model4, newdata=as.matrix(auctions.traind[, -which(colnames(auctions.trainc) == "Competitive.")]))
auctions.traind = auctions.train[, -which(colnames(auctions.train) %in% c("OpenPrice", "ClosePrice"))]
auctions.vald = auctions.val[, -which(colnames(auctions.val) %in% c("OpenPrice", "ClosePrice"))]
xgb_model4 = xgboost(data = as.matrix(auctions.traind[, -which(colnames(auctions.traind) == "Competitive.")]),
label = auctions.traind$Competitive.,
nrounds = 100,
objective = "binary:logistic",
eval_metric = "logloss",
max_depth = 6,
eta = 0.3)
bagging_model4 = bagging(Competitive. ~ ., data = auctions.traind)
rf_model4 = randomForest(Competitive. ~., data = auctions.traind)
xgb_in_sample_pred4 = predict(xgb_model4, newdata=as.matrix(auctions.traind[, -which(colnames(auctions.traind) == "Competitive.")]))
xgb_out_sample_pred4 = predict(xgb_model4, newdata=as.matrix(auctions.vald[, -which(colnames(auctions.vald) == "Competitive.")]))
bagging_in_sample_pred4 = predict(bagging_model4, newdata=auctions.traind)
bagging_out_sample_pred4 = predict(bagging_model4, newdata=auctions.vald)
rf_in_sample_pred4 = predict(rf_model4, newdata=auctions.traind)
rf_out_sample_pred4 = predict(rf_model4, newdata=auctions.vald)
xgb_in_sample_accuracy4 = sum(round(xgb_in_sample_pred4)==auctions.traind$Competitive.)/length(xgb_in_sample_pred4)
xgb_out_sample_accuracy4 = sum(round(xgb_out_sample_pred4)==auctions.vald$Competitive.)/length(xgb_out_sample_pred4)
xgb_imp_var4 = xgb.importance(model = xgb_model4)
bagging_in_sample_accuracy4 = sum(round(bagging_in_sample_pred4)==auctions.traind$Competitive.)/length(bagging_in_sample_pred4)
bagging_out_sample_accuracy4 = sum(round(bagging_out_sample_pred4)==auctions.vald$Competitive.)/length(bagging_out_sample_pred4)
bagging_imp_var4 = varImp(bagging_model4)
rf_in_sample_accuracy4 = sum(round(rf_in_sample_pred4)==auctions.traind$Competitive.)/length(rf_in_sample_pred4)
rf_out_sample_accuracy4 = sum(round(rf_out_sample_pred4)==auctions.vald$Competitive.)/length(rf_out_sample_pred4)
rf_imp_var4 = varImp(rf_model4)
columns_name = c("XGBoost", "Bagging", "Random forest")
train_accuracies4 = c(xgb_in_sample_accuracy4, bagging_in_sample_accuracy4, rf_in_sample_accuracy4)
val_accuracies4 = c(xgb_out_sample_accuracy4, bagging_out_sample_accuracy4, rf_out_sample_accuracy4)
important_var4 = c(
xgb_imp_var4$Feature[which.max(xgb_imp_var4$Gain)],
rownames(bagging_imp_var4)[which.max(bagging_imp_var4$Overall)],
rownames(rf_imp_var4)[which.max(rf_imp_var4$Overall)]
)
table4 = as.data.frame(t(data.frame(
Training_Accuracy = train_accuracies4,
Validation_Accuracy = val_accuracies4,
Most_Important_Variable = important_var4)))
colnames(table4) = columns_name
print(table4)
count_positive = sum(auctions$Competitive.)
percentage_positive = count_positive/nrow(auctions)
print(percentage_positive)
print(table1)
print(table2)
print(table3)
print(table4)
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE)
library(mclust)
library(RColorBrewer)
library(ggplot2)
library(ggplot2)
library(flexclust)
library(ggplot2)
library(flexclust)
library(caret)
library(ggplot2)
library(flexclust)
library(caret)
library(cluster)
library(dplyr)
library(mclust)
data = read.csv("returns.csv")
returns = data[,3:122]
head(data)
data %>% group_by(Industry)
data %>% group_by(Industry) %>% count(Industry)
data %>% group_by(Industry) %>% mean(avg200801,avg200802,avg200803,avg200804,avg200805,avg200806,avg200807,avg200808,avg200809,avg200810,avg2008011,avg200812,avg200901,avg200902,avg200903,avg200904,avg200905,avg200906,avg200907,avg200908,avg200909,avg200910,avg2009011,avg200912,avg201001,avg201002,avg201003,avg201004,avg201005,avg201006,avg201007,avg201008,avg201009,avg201010,avg2010011,avg201012)
data %>% group_by(Industry) %>%
summarise(mean_avg = mean(c(avg200801,avg200802,avg200803,avg200804,avg200805,avg200806,avg200807,avg200808,avg200809,avg200810,avg2008011,avg200812,avg200901,avg200902,avg200903,avg200904,avg200905,avg200906,avg200907,avg200908,avg200909,avg200910,avg2009011,avg200912,avg201001,avg201002,avg201003,avg201004,avg201005,avg201006,avg201007,avg201008,avg201009,avg201010,avg2010011,avg201012), na.rm = TRUE))
data %>% group_by(Industry) %>%
summarise(mean_avg = mean(c(avg200801,avg200802,avg200803,avg200804,avg200805,avg200806,avg200807,avg200808,avg200809,avg200810,avg200811,avg200812,avg200901,avg200902,avg200903,avg200904,avg200905,avg200906,avg200907,avg200908,avg200909,avg200910,avg200911,avg200912,avg201001,avg201002,avg201003,avg201004,avg201005,avg201006,avg201007,avg201008,avg201009,avg201010,avg201011,avg201012), na.rm = TRUE))
knitr::opts_chunk$set(echo = TRUE)
data %>% group_by(Industry) %>% summarise(mean_avg = mean(avg200809, na.rm = TRUE))
data %>% group_by(Industry) %>% summarise(mean_avg = mean(avg200809, na.rm = TRUE))
data %>% group_by(Industry) %>% summarise(mean_avg = mean(avg200809, na.rm = TRUE))
data %>% group_by(Industry) %>% mean(avg200809, na.rm = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(RColorBrewer)
library(ggplot2)
library(flexclust)
library(caret)
library(cluster)
library(dplyr)
library(mclust)
data = read.csv("returns.csv")
returns = data[,3:122]
head(data)
data %>% group_by(Industry) %>% count(Industry)
data %>% group_by(Industry) %>%
summarise(mean_avg = mean(c(avg200801,avg200802,avg200803,avg200804,avg200805,avg200806,avg200807,avg200808,avg200809,avg200810,avg200811,avg200812,avg200901,avg200902,avg200903,avg200904,avg200905,avg200906,avg200907,avg200908,avg200909,avg200910,avg200911,avg200912,avg201001,avg201002,avg201003,avg201004,avg201005,avg201006,avg201007,avg201008,avg201009,avg201010,avg201011,avg201012), na.rm = TRUE))
data %>% group_by(Industry) %>% mean(avg200809, na.rm = TRUE)
data %>% group_by(Industry) %>% summarise(mean_avg = mean(avg200809, na.rm = TRUE))
data %>% group_by(Industry) %>% summarise(mean_avg = mean(avg200809, na.rm = TRUE)) %>% min()
data %>% group_by(Industry) %>% summarise(mean = mean(avg200809, na.rm = TRUE)) %>% summarise(min_mean = min(mean))
data %>% group_by(Industry) %>% summarise(mean = mean(avg200809, na.rm = TRUE)) %>% summarise(min_mean = min(mean)) %>% select(Industry)
data %>%
group_by(Industry) %>%
summarise(mean = mean(avg200809, na.rm = TRUE)) %>%
arrange(mean) %>%
slice(1)
h_clust = hclust(data, method="ward.D2")
h_clust = hclust(dist(data), method="ward.D2")
ggdendrogram(dendro, theme_dendro = FALSE)
plot(merge_heights, type = "b", xlab = "Number of Clusters", ylab = "Height", main = "Scree Plot")
dendro = as.dendrogram(h_clust)
ggdendrogram(dendro, theme_dendro = FALSE)
merge_heights = h_clust$height
plot(merge_heights, type = "b", xlab = "Number of Clusters", ylab = "Height", main = "Scree Plot")
library(ggdendro)
install.packages("ggdendro")
install.packages("ggdendro")
library(ggdendro)
ggdendrogram(dendro, theme_dendro = FALSE)
h_clust = hclust(dist(returns, method ="euclidean"), method = "ward.D2")
plot(h_clust, hang=-1, main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
plot(h_clust, main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
plot(h_clust, hang=-1,main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height),dissimilarity = rev(h_clust$height))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line(lwd=2) +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
dendro = as.dendrogram(h_clust)
ggdendrogram(dendro, theme_dendro = FALSE)
plot(h_clust, hang=-1,main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
dendro = as.dendrogram(h_clust)
ggdendrogram(dendro, theme_dendro = FALSE)
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height),dissimilarity = rev(h_clust$height))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line(lwd=2) +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
h_clust_heights = data.frame(number_clust = length(h_clust$height), dissimilarity = h_clust$height)
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
h_clust_heights = data.frame(number_clust = length(h_clust$height), dissimilarity = h_clust$height)
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height), dissimilarity =h_clust$height)
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
AVG = data %>% group_by(Industry) %>%
summarise(mean_avg = mean(c(avg200801,avg200802,avg200803,avg200804,avg200805,avg200806,avg200807,avg200808,avg200809,avg200810,avg200811,avg200812,avg200901,avg200902,avg200903,avg200904,avg200905,avg200906,avg200907,avg200908,avg200909,avg200910,avg200911,avg200912,avg201001,avg201002,avg201003,avg201004,avg201005,avg201006,avg201007,avg201008,avg201009,avg201010,avg201011,avg201012), na.rm = TRUE))
AVG
ggplot(AVG, x=Industry, y=mean_avg)+
geom_bar()
ggplot(AVG, aes(x=Industry, y=mean_avg))+
geom_bar()
ggplot(AVG, aes(y=mean_avg))+
geom_bar()
ggplot(AVG, aes(x = Industry, y = mean_avg)) +
geom_bar(stat = "identity", fill = "blue")
ggplot(AVG, aes(x = Industry, y = mean_avg)) +
geom_bar(stat = "identity")+
labs(title = "Average Stock Returns by Industry (01/2008 - 12/2010)",
x = "Industry", y = "Average Return") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(AVG, aes(x = Industry, y = mean_avg, fill=Industry)) +
geom_bar(stat = "identity")+
labs(title = "Average Stock Returns by Industry (01/2008 - 12/2010)",
x = "Industry", y = "Average Return") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
xlim(0, 100)
xlim(0, 100)
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100))
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height),dissimilarity = rev(h_clust$height))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100))
cutree(tree=h_clust, k=8)
clusters
clusters = cutree(tree=h_clust, k=8)
clusters
data$clusters = clusters
data
data$clusters
data$Industry
data$clusters = clusters
data %>% groupby(clusters) %>% count(clusters)
data %>% group_by(clusters) %>% count(clusters)
data %>% group_by(Industry, clusters) %>% count(clusters)
data %>% group_by(clusters) %>% summarize(avg_oct_2008=mean(avg200810), avg_mar_2009=mean(avg200903))
plot(h_clust, hang=-1,main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height),dissimilarity = rev(h_clust$height))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100))
set.seed(2407)
k_means = kmeans(returns, centers = 8, iter.max = 100)
data$k_means = k_means$cluster
data
data$k_means
k_means$centers
centroid_mar_2009 = k_means$centers$avg200909
centroid_oct_2008 = k_means$centers["avg200810"]
centroid_mar_2009 = k_means$centers["avg200909"]
centroid_oct_2008
centroid_mar_2009
k_means$centers
k_means$centers["avg200603"]
k_means$centers$avg200603
k_means$centers[1]
k_means$centers
k_means$centers[:,1]
k_means$centers[,1]
k_means$centers[,"avg200810"]
centroid_oct_2008 = k_means$centers[,"avg200810"]
centroid_mar_2009 = k_means$centers[,"avg200909"]
centroid_oct_2008
centroid_mar_2009
table(data$clusters, data$k_means)
distances <- dist(returns)
sil_hc <- silhouette(h_clust,distances)
distances <- dist(returns)
sil_hc <- silhouette(clusters,distances)
plot(sil_hc, col=1:selected.k, border=NA)
plot(sil_hc, col=1:8, border=NA)
sil_hc
sil_hc %>% group_by(cluster) %>% summarize(MeanSilhouetteScore = mean(sil_width))
sil_hc <- data.frame(silhouette(clusters,distances))
plot(sil_hc, col=1:8, border=NA)
sil_hc <- silhouette(clusters,distances)
plot(sil_hc, col=1:8, border=NA)
sil_hc_df = data.frame(sil_hc)
sil_hc_df %>% group_by(cluster) %>% summarize(MeanSilhouetteScore = mean(sil_width))
distances <- dist(returns)
sil_hc <- silhouette(clusters,distances)
sil_hc_df = data.frame(sil_hc)
plot(sil_hc, col=1:8, border=NA)
sil_hc_df %>% summarize(MeanSilhouetteScore = mean(sil_width))
sil_km <- silhouette(k_means,distances)
sil_km_df = data.frame(sil_km)
sil_km <- silhouette(k_means,distances)
sil_km <- silhouette(data$k_means,distances)
sil_km_df = data.frame(sil_km)
plot(sil_km, col=1:8, border=NA)
sil_km_df %>% group_by(k_means) %>% summarize(MeanSilhouetteScore = mean(sil_width))
sil_km_df %>% group_by(cluster) %>% summarize(MeanSilhouetteScore = mean(sil_width))
sil_km_df %>% summarize(MeanSilhouetteScore = mean(sil_width))
library(RColorBrewer)
library(ggplot2)
library(flexclust)
library(caret)
library(cluster)
library(dplyr)
library(mclust)
library(ggdendro)
df = read.csv("final_df.csv")
X_train <- subset(df, Year <= 2022)
X_test <- subset(df, Year >= 2023)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Docs/MIT/Fall/A-lab/analyticslab")
df = read.csv("DATA/final_df.csv")
X_train <- subset(df, Year <= 2022)
X_test <- subset(df, Year >= 2023)
# Separating the target variable
y_train <- X_train$CompanySpend
y_test <- X_test$CompanySpend
# Dropping unnecessary columns
X_train <- select(X_train, -c(CompanySpend, Company, Year))
X_test <- select(X_test, -c(CompanySpend, Company, Year))
h_clust = hclust(dist(X_train, method ="euclidean"), method = "ward.D2")
plot(h_clust, hang=-1,main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height),dissimilarity = rev(h_clust$height))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity"))
+
xlim(0, 100))
+
xlim(0, 100))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100))
# Scale the training data
X_scaled_train <- scale(X_train)
# If you want to use the same scaling parameters for the test data,
# you'll need to compute the means and standard deviations from the training data
train_means <- apply(X_train, 2, mean)
train_sds <- apply(X_train, 2, sd)
# Then use these values to scale the test data manually
# (Assuming X_test is your testing dataset)
X_scaled_test <- sweep(sweep(X_test, 2, train_means, "-"), 2, train_sds, "/")
h_clust = hclust(dist(X_train_scaled, method ="euclidean"), method = "ward.D2")
plot(h_clust, hang=-1,main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height),dissimilarity = rev(h_clust$height))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100))
# Scale the training data
X_scaled_train <- scale(X_train)
h_clust = hclust(dist(X_scaled_train, method ="euclidean"), method = "ward.D2")
plot(h_clust, hang=-1,main="Hierarchical Clustering Dendrogram", xlab="Stock Index", ylab="Distance")
h_clust_heights = data.frame(number_clust = seq_along(h_clust$height),dissimilarity = rev(h_clust$height))
print(ggplot(h_clust_heights, aes(x=number_clust, y=dissimilarity)) +
geom_line() +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100))
